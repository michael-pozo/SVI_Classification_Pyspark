{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976396c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/opt/homebrew/Caskroom/miniforge/base/envs/Qoya/lib/python3.8/site-packages/sklearn/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, StackingClassifier\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscriminant_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuadraticDiscriminantAnalysis\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score, f1_score, accuracy_score, plot_confusion_matrix,\\\n\u001b[1;32m     14\u001b[0m classification_report\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, GridSearchCV,\\\n\u001b[1;32m     16\u001b[0m cross_val_score, RandomizedSearchCV, KFold\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, Normalizer, RobustScaler, MinMaxScaler, MaxAbsScaler,\\\n\u001b[1;32m     18\u001b[0m QuantileTransformer, PowerTransformer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/opt/homebrew/Caskroom/miniforge/base/envs/Qoya/lib/python3.8/site-packages/sklearn/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats \n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, plot_confusion_matrix,\\\n",
    "classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,\\\n",
    "cross_val_score, RandomizedSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler, MinMaxScaler, MaxAbsScaler,\\\n",
    "QuantileTransformer, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ada6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316f38a",
   "metadata": {},
   "source": [
    "## Census Data Import and Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76603db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ST</th>\n",
       "      <th>STATE</th>\n",
       "      <th>ST_ABBR</th>\n",
       "      <th>STCNTY</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>AREA_SQMI</th>\n",
       "      <th>E_TOTPOP</th>\n",
       "      <th>M_TOTPOP</th>\n",
       "      <th>...</th>\n",
       "      <th>EP_ASIAN</th>\n",
       "      <th>MP_ASIAN</th>\n",
       "      <th>EP_AIAN</th>\n",
       "      <th>MP_AIAN</th>\n",
       "      <th>EP_NHPI</th>\n",
       "      <th>MP_NHPI</th>\n",
       "      <th>EP_TWOMORE</th>\n",
       "      <th>MP_TWOMORE</th>\n",
       "      <th>EP_OTHERRACE</th>\n",
       "      <th>MP_OTHERRACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>36001</td>\n",
       "      <td>Albany</td>\n",
       "      <td>36001000100</td>\n",
       "      <td>Census Tract 1, Albany County, New York</td>\n",
       "      <td>0.914079</td>\n",
       "      <td>2029</td>\n",
       "      <td>355</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>36001</td>\n",
       "      <td>Albany</td>\n",
       "      <td>36001000201</td>\n",
       "      <td>Census Tract 2.01, Albany County, New York</td>\n",
       "      <td>0.237787</td>\n",
       "      <td>3263</td>\n",
       "      <td>759</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>36001</td>\n",
       "      <td>Albany</td>\n",
       "      <td>36001000202</td>\n",
       "      <td>Census Tract 2.02, Albany County, New York</td>\n",
       "      <td>0.556562</td>\n",
       "      <td>2153</td>\n",
       "      <td>488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>36001</td>\n",
       "      <td>Albany</td>\n",
       "      <td>36001000301</td>\n",
       "      <td>Census Tract 3.01, Albany County, New York</td>\n",
       "      <td>0.254634</td>\n",
       "      <td>3016</td>\n",
       "      <td>602</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>36001</td>\n",
       "      <td>Albany</td>\n",
       "      <td>36001000302</td>\n",
       "      <td>Census Tract 3.02, Albany County, New York</td>\n",
       "      <td>1.968324</td>\n",
       "      <td>2931</td>\n",
       "      <td>509</td>\n",
       "      <td>...</td>\n",
       "      <td>8.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>7.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ST     STATE ST_ABBR  STCNTY  COUNTY         FIPS  \\\n",
       "0  36  New York      NY   36001  Albany  36001000100   \n",
       "1  36  New York      NY   36001  Albany  36001000201   \n",
       "2  36  New York      NY   36001  Albany  36001000202   \n",
       "3  36  New York      NY   36001  Albany  36001000301   \n",
       "4  36  New York      NY   36001  Albany  36001000302   \n",
       "\n",
       "                                     LOCATION  AREA_SQMI  E_TOTPOP  M_TOTPOP  \\\n",
       "0     Census Tract 1, Albany County, New York   0.914079      2029       355   \n",
       "1  Census Tract 2.01, Albany County, New York   0.237787      3263       759   \n",
       "2  Census Tract 2.02, Albany County, New York   0.556562      2153       488   \n",
       "3  Census Tract 3.01, Albany County, New York   0.254634      3016       602   \n",
       "4  Census Tract 3.02, Albany County, New York   1.968324      2931       509   \n",
       "\n",
       "   ...  EP_ASIAN  MP_ASIAN  EP_AIAN  MP_AIAN  EP_NHPI  MP_NHPI  EP_TWOMORE  \\\n",
       "0  ...       4.9       3.6      0.0      1.7      0.0      1.7        10.0   \n",
       "1  ...       2.7       3.2      0.0      1.1      0.0      1.1         6.2   \n",
       "2  ...       0.0       1.6      0.0      1.6      0.0      1.6         4.6   \n",
       "3  ...      13.0       8.5      0.0      1.2      0.0      1.2         3.9   \n",
       "4  ...       8.8       8.6      0.0      1.2      0.0      1.2         7.1   \n",
       "\n",
       "   MP_TWOMORE  EP_OTHERRACE  MP_OTHERRACE  \n",
       "0         5.0           0.2           0.4  \n",
       "1         4.8           0.0           1.1  \n",
       "2         4.1           0.0           1.6  \n",
       "3         5.5           0.0           1.2  \n",
       "4         6.1           0.4           0.7  \n",
       "\n",
       "[5 rows x 158 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_df = pd.read_csv('data/NewYork.csv')\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052c6eb",
   "metadata": {},
   "source": [
    "From reading through the data dictionary (included in the Data folder), I know that all columns beginning in M are the margins of error for each population estimate. Additionally, I can drop all the percentile transformation columns and any unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebbe1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['M', 'EP', 'SP', 'F']\n",
    "\n",
    "for col_name in census_df.columns:\n",
    "    for string in to_drop:\n",
    "        if col_name.startswith(string):\n",
    "            census_df.drop(columns=col_name, inplace=True)\n",
    "            break\n",
    "\n",
    "census_df.drop(columns = ['RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'E_DAYPOP',\n",
    "                          'E_AFAM', 'E_HISP', 'E_ASIAN', 'E_AIAN', 'E_NHPI','E_TWOMORE', \n",
    "                          'E_OTHERRACE'], inplace = True)\n",
    "census_df.drop(columns = ['ST', 'STATE', 'ST_ABBR', 'STCNTY', 'LOCATION'], inplace = True)\n",
    "census_df.rename({'RPL_THEMES': 'SVI_Rank'}, inplace = True, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e5d7948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['COUNTY', 'AREA_SQMI', 'E_TOTPOP', 'E_HU', 'E_HH', 'E_POV150',\n",
       "       'E_UNEMP', 'E_HBURD', 'E_NOHSDP', 'E_UNINSUR', 'E_AGE65', 'E_AGE17',\n",
       "       'E_DISABL', 'E_SNGPNT', 'E_LIMENG', 'E_MINRTY', 'E_MUNIT', 'E_MOBILE',\n",
       "       'E_CROWD', 'E_NOVEH', 'E_GROUPQ', 'SVI_Rank', 'E_NOINT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279b3e9",
   "metadata": {},
   "source": [
    "Additional Notes From Data Dictionary\n",
    "\n",
    "* Tracts with zero estimates for total population (N = 645 for the U.S.) were removed during the ranking process. These tracts were added back to the SVI databases after ranking. The TOTPOP field value is 0, but the percentile ranking fields (RPL_THEME1, RPL_THEME2, RPL_THEME3, RPL_THEME4, and RPL_THEMES) were set to -999.\n",
    "\n",
    "* For tracts with > 0 TOTPOP, a value of -999 in any field either means the value was unavailable from the original census data or we could not calculate a derived value because of unavailable census data.\n",
    "\n",
    "* Any cells with a -999 were not used for further calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ade7e",
   "metadata": {},
   "source": [
    "I'll remove any rows with SVI Rank of -999 since they contain no data. Values of 999 in rows containing data will be imputed later on after the Train-Test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ef257",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_SVI = census_df.loc[census_df['SVI_Rank'] == -999]\n",
    "census_df = census_df.drop(index = missing_SVI.index).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd563e",
   "metadata": {},
   "source": [
    "## COVID Data Import and Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = pd.read_csv('data/United_States_COVID-19_Community_Levels_by_County.csv')\n",
    "\n",
    "#Data is limited to New York to align with the census data\n",
    "#Census data is dated 2020, so I used the earliest update in the COVID data\n",
    "\n",
    "ny_covid_df = covid_df[covid_df['state'] == 'New York']\n",
    "ny_covid_df = ny_covid_df[ny_covid_df['date_updated'] == '2022-02-24' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff462c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "sns.barplot(x = 'covid-19_community_level', y = 'covid_cases_per_100k', data = ny_covid_df,\n",
    "            estimator = np.sum, order = ['Low', 'Medium', 'High'])\n",
    "ax.set_xlabel('COVID-19 Community Level')\n",
    "ax.set_ylabel('Number of Cases per 100k')\n",
    "ax.set_title('COVID-19 Cases by COVID-19 Community Level');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e51b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data to be merged with census_df\n",
    "\n",
    "ny_county_covid = ny_covid_df[['county', 'covid-19_community_level']]\n",
    "ny_county_covid['county'] = ny_county_covid['county'].str.split().str[0]\n",
    "ny_county_covid['county'].replace(['New', 'St.'], ['New York', 'St. Lawrence'], inplace = True)\n",
    "ny_county_covid.rename(columns = {'county': 'COUNTY'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f777b",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa54f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging census_df and ny_county_covid to create final dataframe for analysis \n",
    "\n",
    "joint_df = census_df.merge(ny_county_covid, how = 'left', on = 'COUNTY')\n",
    "joint_df = joint_df.rename(columns = {'covid-19_community_level': 'target'})\n",
    "joint_df = joint_df.drop(columns = ['SVI_Rank', 'COUNTY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b26ab",
   "metadata": {},
   "source": [
    "Referring back to the data dictionary, I know I need to mark the missing values for imputation later on. I will also encode my target into a numeric value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df.replace(999, np.nan, inplace = True )\n",
    "joint_df['target'] = joint_df['target'].map({'Low':0, 'Medium': 1, 'High': 2})\n",
    "joint_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99feec1",
   "metadata": {},
   "source": [
    "Begin EDA by visualizing the relationship between the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(hue = 'target', data = joint_df, height = 1.75, corner = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84678da7",
   "metadata": {},
   "source": [
    "Many of the variables are colinear, which makes sense becuase they are all population estimates. Use of regularization techniques will be important. I also notice that the most of the variables are not normally distributed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e906dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "sns.countplot(x = 'target', data = joint_df)\n",
    "ax.set_xticklabels(['Low', 'Medium', 'High'])\n",
    "ax.set_xlabel('COVID-19 Community Level')\n",
    "ax.set_ylabel('Number of Census Tracts')\n",
    "ax.set_title('COVID-19 Community Levels in New York Census Tracts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c070ad1",
   "metadata": {},
   "source": [
    "The target is very imbalanced. Use of an oversampling technique will be necessary to train the model to predict the minority class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "sns.scatterplot(x = 'AREA_SQMI', y = 'E_TOTPOP', hue = 'target', data = joint_df, legend = 'brief')\n",
    "ax.set_xlabel('Census Tract Area (SQMI)')\n",
    "ax.set_ylabel('Census Tract Population Sample')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "label_dict = {0: 'Low', 1: 'Medium', 2: 'High'}\n",
    "new_labels = [label_dict.get(int(label), label) for label in labels]\n",
    "ax.legend(handles=handles, labels=new_labels, title='Community COVID Level')\n",
    "ax.set_title('Estimated Population by Census Tract Area');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d8beed",
   "metadata": {},
   "source": [
    "New York is densely populated. The majority of the population lives in census tracts that are very small. Given what we know about the spread of COVID, my hypothesis is that both of these factors will be influential in predicting community COVID levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81c78e",
   "metadata": {},
   "source": [
    "## Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = joint_df.drop(columns = 'target')\n",
    "y = joint_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = .25, random_state = 42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0e6f9",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0b988",
   "metadata": {},
   "source": [
    "In this model and all subsequent models, we will be setting up pipelines to streamline the model building process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f44076",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('imputer', SimpleImputer(strategy=\"median\")), ('std_scaler', StandardScaler()),\n",
    "        ('lr', LogisticRegression(solver = 'saga', random_state=42))]\n",
    "\n",
    "baseline_pipe = Pipeline(steps)\n",
    "baseline_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb39164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom cross validation function for future use \n",
    "\n",
    "def cross_validation(X_train, y_train, estimator, num_split = 5):\n",
    "    \n",
    "    X_train = X_train.values\n",
    "    y_train = y_train.values\n",
    "    \n",
    "    score_train_list = []\n",
    "    score_val_list = []\n",
    "    \n",
    "    for train_index, valid_index in KFold(n_splits = num_split).split(X_train):\n",
    "        \n",
    "        # train and validation splitting \n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[valid_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[valid_index]\n",
    "\n",
    "        estimator.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # now how did we do?\n",
    "        accuracy_train = estimator.score(X_train_fold, y_train_fold)\n",
    "        accuracy_val = estimator.score(X_val_fold, y_val_fold)\n",
    "        score_val_list.append(accuracy_val)\n",
    "        score_train_list.append(accuracy_train)\n",
    "    \n",
    "    return {'train': np.mean(score_train_list), 'validation': np.mean(score_val_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d26aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess baseline model performance\n",
    "\n",
    "print(cross_validation(X_train, y_train, baseline_pipe))\n",
    "plot_confusion_matrix(baseline_pipe, X_test, y_test)\n",
    "y_pred = baseline_pipe.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5c800",
   "metadata": {},
   "source": [
    "Baseline model is 82.4% accurate, but misses almost all of the high covid level census tracts. Given the business problem, it is better to accurately classify more high risk areas, even if there are more false positives. In future models, I will give larger consideration to class 2 recall as a metric of success "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6c882a",
   "metadata": {},
   "source": [
    "## Model 2: Logistic Regression with Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc154b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_2 = [('imputer', SimpleImputer(strategy=\"median\")),\n",
    "           ('sampler', SMOTE(sampling_strategy = 'minority', random_state =42)),\n",
    "           ('std_scaler', StandardScaler()),\n",
    "        ('lr', LogisticRegression(solver = 'saga', random_state=42))]\n",
    "\n",
    "sm_pipe = Pipeline(steps_2)\n",
    "sm_model = sm_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess model performance \n",
    "print(cross_validation(X_train, y_train, sm_pipe))\n",
    "plot_confusion_matrix(sm_model, X_test, y_test)\n",
    "y_pred = sm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c9640",
   "metadata": {},
   "source": [
    "Overall accuracy drops to 75%, but class 2 recall is up to 89%. I will now consider this my best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be60fe",
   "metadata": {},
   "source": [
    "## Model 3: Tuned Logistic Regression with Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa36043",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [10 ** i for i in range(-3, 4)]\n",
    "tol_values = [1, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "max_iter_values = np.arange(500, 20000, 500)\n",
    "\n",
    "params = {'lr__penalty': ['l1', 'l2'],\n",
    "          'lr__C': c_values,\n",
    "         'lr__max_iter': max_iter_values,\n",
    "         'lr__tol': tol_values}\n",
    "\n",
    "cv = GridSearchCV(estimator = sm_model, param_grid = params, cv = 5, n_jobs = -1)\n",
    "cv.fit(X_train, y_train)\n",
    "best_sm_model = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_validation(X_train, y_train, best_sm_model))\n",
    "plot_confusion_matrix(best_sm_model, X_test, y_test)\n",
    "y_pred = best_sm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbcc4f",
   "metadata": {},
   "source": [
    "The tuned model is slightly more accurate than the original oversampled logistic regression, but lacks the high recall value for class 2. I will stick with model 2 as my best model for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a5bd9",
   "metadata": {},
   "source": [
    "## Model 4: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce81062",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_4 = [('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('sampler', SMOTE(sampling_strategy = 'minority', random_state =42)),   \n",
    "        ('dt', DecisionTreeClassifier(max_depth = 10, random_state = 42))]\n",
    "\n",
    "dt_pipe = Pipeline(steps_4)\n",
    "dt_model = dt_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0738229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess model performance\n",
    "\n",
    "print(cross_validation(X_train, y_train, dt_model))\n",
    "plot_confusion_matrix(dt_model, X_test, y_test)\n",
    "y_pred = dt_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa93605",
   "metadata": {},
   "source": [
    "Decision Tree produces a high training score, but also a high amount of variance. I can tell the model is overfitting by comparing the train and validation scores. I will attempt to tune the model to reduce the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2316fe",
   "metadata": {},
   "source": [
    "## Model 5: Tuned Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70742c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_values = range(2,11)\n",
    "min_sample_leaf_values = range(2,6)\n",
    "max_feature_values = range(2,22)\n",
    "criterion_values = ['gini', 'entropy']\n",
    "splitter_values = ['best', 'random']\n",
    "class_weight_values = ['balanced', None]\n",
    "\n",
    "params = {'dt__max_depth': max_depth_values,\n",
    "           'dt__min_samples_leaf': min_sample_leaf_values,\n",
    "          'dt__max_features': max_feature_values,\n",
    "          'dt__criterion': criterion_values,\n",
    "         'dt__splitter': splitter_values,\n",
    "         'dt__class_weight': class_weight_values}\n",
    "\n",
    "dt_cv = GridSearchCV(estimator = dt_model, param_grid = params, cv = 5, n_jobs = -1)\n",
    "dt_cv.fit(X_train, y_train)\n",
    "best_dt_model = dt_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess model performance\n",
    "\n",
    "print(cross_validation(X_train, y_train, best_sm_model))\n",
    "plot_confusion_matrix(best_sm_model, X_test, y_test)\n",
    "y_pred = best_sm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f78363",
   "metadata": {},
   "source": [
    "There is a small reduction in the overfitting, but the accuracy and class 2 recall is still lower than my best model, so I will keep going."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054af11",
   "metadata": {},
   "source": [
    "## Model 6: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f471d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('sampler', SMOTE(sampling_strategy = 'minority', random_state =42)),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=150, random_state = 42))])\n",
    "\n",
    "rf_model = rf_pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e42ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_validation(X_train, y_train, rf_model))\n",
    "plot_confusion_matrix(rf_model, X_test, y_test)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fda793",
   "metadata": {},
   "source": [
    "There is improvement in the overall accuracy, but the model is extremely overfitted. I will try to tune it to improve accuracy score while also managing hyperparameters to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc601419",
   "metadata": {},
   "source": [
    "## Model 7: Tuned Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'rf__n_estimators': [10, 50, 100, 200, 500],\n",
    "          'rf__max_depth': [5, 10, 20, None],\n",
    "          'rf__min_samples_split': [2, 5, 10],\n",
    "          'rf__min_samples_leaf': [1, 2, 4],\n",
    "          'rf__max_features': ['auto', 'sqrt', 'log2', None]}\n",
    "\n",
    "rf_cv = GridSearchCV(estimator = rf_model, param_grid = params, cv = 5, n_jobs = -1)\n",
    "rf_cv.fit(X_train, y_train)\n",
    "best_rf_model = rf_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148467ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#play around with the max depth to reduce overfitting while maximizing accuracy and class 2 recall\n",
    "\n",
    "# best_rf_model.set_params(rf__max_depth = 10)\n",
    "# best_rf_model.set_params(rf__max_depth = 5)\n",
    "best_rf_model.set_params(rf__max_depth = 6)\n",
    "# best_rf_model.set_params(rf__max_depth = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2aa1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess model performance \n",
    "\n",
    "print(cross_validation(X_train, y_train, best_rf_model))\n",
    "plot_confusion_matrix(best_rf_model, X_test, y_test)\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc5e53",
   "metadata": {},
   "source": [
    "Random forest is capable of achieving a higher accuracy than the logistic regression model, but is prone to overfitting. By adjusting the max depth, I was able to reduce the overfitting, but the best scores were 73% accuracy and 63% class 2 recall. My current best model is still superior. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa3afd",
   "metadata": {},
   "source": [
    "## Model 8: Gradient Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('sampler', SMOTE(sampling_strategy = 'minority', random_state =42)),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    ('gb', XGBClassifier(random_state = 42))])\n",
    "\n",
    "gb_model = gb_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_validation(X_train, y_train, gb_model))\n",
    "plot_confusion_matrix(gb_model, X_test, y_test)\n",
    "y_pred = gb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843704c",
   "metadata": {},
   "source": [
    "Similar to the random forest, the validation accuracy score was high, but the model is overfitting. I will attempt to tune it to increase accuracy and reduce variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a64ce9",
   "metadata": {},
   "source": [
    "## Model 9: Tuned Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'gb__n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "          'gb__max_depth': [1, 2, 3, 4, 5],\n",
    "          'gb__learning_rate': [.01, .03, .05, .07, .1],\n",
    "          'gb__subsample': [0.5, 0.75, 1]}\n",
    "\n",
    "gb_cv = GridSearchCV(estimator = gb_model, param_grid = params, cv = 5, n_jobs = -1)\n",
    "gb_cv.fit(X_train, y_train)\n",
    "best_gb_model = gb_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87387cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess model\n",
    "\n",
    "print(cross_validation(X_train, y_train, best_gb_model))\n",
    "plot_confusion_matrix(best_gb_model, X_test, y_test)\n",
    "y_pred = best_gb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d02e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to reduce overfitting\n",
    "\n",
    "best_gb_model_overfit = best_gb_model.set_params(gb__max_depth = 5)\n",
    "best_gb_model_overfit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835103b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess model\n",
    "\n",
    "print(cross_validation(X_train, y_train, best_gb_model_overfit))\n",
    "plot_confusion_matrix(best_gb_model_overfit, X_test, y_test)\n",
    "y_pred = best_gb_model_overfit.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb0879",
   "metadata": {},
   "source": [
    "Tuned model is still very overfitted. I was able to reduce the overfitting by reducing the max depth and decreasing the learning rate, but ultimately the accuracy was only a couple points better than my current best model, but with a 40% lower class 2 recall. I will continue to use the oversampled logistic regression as my best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a66161",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(best_gb_model_overfit.named_steps['gb'].feature_importances_,\n",
    "             index = X_train.columns).sort_values(ascending = False)\n",
    "\n",
    "top_5 = feat_imp.head()\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(x = top_5.index, y = top_5.values)\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Feature Importance')\n",
    "ax.set_title('Top 5 Most Important Features')\n",
    "new_labels = ['Area', 'Mobile Homes', 'Poverty','Low English', 'Crowded Home']\n",
    "ax.set_xticklabels(new_labels);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62642e",
   "metadata": {},
   "source": [
    "This visual tells us which features are the most influential in determining community COVID level and will be part of recommendation to the stakeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2c6de",
   "metadata": {},
   "source": [
    "## Model 10: K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('sampler', SMOTE(sampling_strategy = 'minority', random_state =42)),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_jobs = -1) )])\n",
    "\n",
    "knn_model = knn_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_validation(X_train, y_train, knn_model))\n",
    "plot_confusion_matrix(knn_model, X_test, y_test)\n",
    "y_pred = knn_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39005213",
   "metadata": {},
   "source": [
    "Similar to XGB Classifier, KNN produces improved accuracy scores, but is overfitted and has lower class 2 recall. I will attempt to tune this model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5d4f5",
   "metadata": {},
   "source": [
    "## Model 11: Tuned K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'knn__weights': ['uniform', 'distance'],\n",
    "          'knn__n_neighbors': range(5, 21),\n",
    "          'knn__leaf_size': [5, 10, 20, 30, 40, 50]}\n",
    "\n",
    "knn_cv = GridSearchCV(estimator = knn_model, param_grid = params, cv = 5, n_jobs = -1)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "best_knn_model = knn_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_validation(X_train, y_train, best_knn_model))\n",
    "plot_confusion_matrix(best_knn_model, X_test, y_test)\n",
    "y_pred = best_knn_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb447a4",
   "metadata": {},
   "source": [
    "Model continue to overfit and scores are not an improvement over the reigning best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b61083",
   "metadata": {},
   "source": [
    "## Model 12: Stacked XGB Classifier and Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d3e87",
   "metadata": {},
   "source": [
    "I will choose my two best models to stack together. The first model will be my current best model, the oversampled logistic regression. The second model will be the tuned gradient boost with the overfitting reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('lr', sm_model),\n",
    "             ('gb', best_gb_model_overfit)]\n",
    "\n",
    "sc = StackingClassifier(estimators)\n",
    "lr_gb_model = sc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess model performance \n",
    "print(cross_validation(X_train, y_train, lr_gb_model))\n",
    "plot_confusion_matrix(lr_gb_model, X_test, y_test)\n",
    "y_pred = lr_gb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f022bf",
   "metadata": {},
   "source": [
    "One of the highest accuracy scores, but the model is still overfitting and the class 2 recall is very low at 23%. I will use the oversampled logistic regression as my final model and use it to see if I can find any insights for my stakeholder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee1e417",
   "metadata": {},
   "source": [
    "## Experimenting with Dropping some Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013302a",
   "metadata": {},
   "source": [
    "I will use my feature importance from the XGB Classifier to find the least important features and drop them from my best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp\n",
    "joint_df_2 = joint_df.drop(columns = ['E_HBURD', 'E_UNEMP', 'E_TOTPOP', 'E_DISABL', 'E_AGE17'])\n",
    "\n",
    "X_2 = joint_df_2.drop(columns = 'target')\n",
    "y_2 = joint_df_2['target']\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, stratify = y, test_size = .25, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91327c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess model performance\n",
    "sm_model.fit(X_train_2, y_train_2)\n",
    "print(cross_validation(X_train_2, y_train_2, sm_model))\n",
    "plot_confusion_matrix(sm_model, X_test_2, y_test_2)\n",
    "y_pred = sm_model.predict(X_test_2)\n",
    "print(classification_report(y_test_2, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe3453",
   "metadata": {},
   "source": [
    "Resulting model was not improved in any of the key criteria so I will leave all features in.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e12e252",
   "metadata": {},
   "source": [
    "## Drawing Insights from Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8597a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_copy = joint_df.copy()\n",
    "\n",
    "impute = SimpleImputer(strategy = 'median')\n",
    "imputed_copy = impute.fit_transform(joint_copy)\n",
    "imputed_copy = pd.DataFrame(imputed_copy, columns = joint_copy.columns)\n",
    "\n",
    "imp_low = imputed_copy.loc[imputed_copy['target'] == 0]\n",
    "imp_medium = imputed_copy.loc[imputed_copy['target'] == 1]\n",
    "imp_high = imputed_copy.loc[imputed_copy['target'] == 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b541b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.distplot(imp_low['E_POV150'], kde = True, label = 'Low')\n",
    "sns.distplot(imp_medium['E_POV150'], kde = True, label = 'Medium')\n",
    "sns.distplot(imp_high['E_POV150'], kde = True, label = 'High')\n",
    "ax.set_xlabel('Estimated Population Living 150% Below Poverty Level')\n",
    "ax.set_title('Distribution of Estimated Population Living 150% Below Poverty Level')\n",
    "ax.legend(title = \"Community COVID Level\")\n",
    "ax.set_xlim(0,5000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stats.f_oneway(imp_low['E_POV150'], imp_medium['E_POV150'], imp_high['E_POV150']) \n",
    "f_stat, p = result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f2532",
   "metadata": {},
   "source": [
    "The model predicts poverty levels being a highly influential featue. Looking back at the distributions of poverty levels across the three classes, I can see that the high COVID level class has a higher mean poverty level. Using an ANOVA test, I compared the means at the 95% confidence level and found that they are meanigfully different. Monitoring local poverty rates as an indicator of vulnerable populations will be part of my recommendation to the stakeholders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(figsize = (10,6))\n",
    "sns.boxplot(x = 'preds', y = 'E_MOBILE', data = preds_df, labels = ['Low', 'Medium'])\n",
    "ax.set_xlabel('Estimated Population Living 150% Below Poverty Level')\n",
    "ax.set_title('Distribution of Estimated Population Living 150% Below Poverty Level')\n",
    "ax.legend(title = \"Predicted Community COVID Level\")\n",
    "ax.set_xticklabels(['Low', 'Medium', 'Hihg'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a2c18",
   "metadata": {},
   "source": [
    "The population living in mobile homes was a feature that was surprisingly influential in classifying community COVID levels. Looking at the distribution of mobile homes by predicted COVID levels, I can see a clear distinction between High COVID areas and the other two classes. Monitoring the local mobile home population as an indicator of vulnerable populations will be the second part of my recommendation to the stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc053e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qoya",
   "language": "python",
   "name": "qoya"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
